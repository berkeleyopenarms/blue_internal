# ROBOT SPECIFIC SETTINGS ====================================================================
# first of all, you will have to adapt all the link and joint names to your robot.
# you might also have to adjust parameters to your robot, their meaning is explained below.

# the robot description used by the algorithm
robot_description_name: robot_description

# we do not render the entire robot, but only it's arms. below we 
# specify the links at which we start rendering
rendering_root_left: L_SHOULDER
rendering_root_right: R_SHOULDER

# we publish the tf of the estimated robot pose under the prefix /estimated.
# below we specify at which frame this estimated tf is connected to the
# standard tf
tf_connecting_frame: XTION_RGB

# often, the timestamp of the depth images is not quite correct.
# we have true_timestamp = image_timestamp - camera delay
camera_delay: 0.033

# below you find the paramters of the observation and state transition models,
# as introduced in [1].

# the noise magnitude on joint encoders is typically very low, for simplicity of analysis
# we set it to zero. you will not need to adapt these parameters, unless your robot
# has very noisy joint encoders.
joint_observation:
  joint_sigmas: [
    # neck     # eyes     # left arm  # right arm  # left hand  # right hand
    B_HN: 0.,  L_EP: 0.,  L_SFE: 0.,  R_SFE: 0.,   L_FR:  0.,   R_FR:  0., 
    B_HT: 0.,  L_ET: 0.,  L_SAA: 0.,  R_SAA: 0.,   L_LF:  0.,   R_LF:  0., 
    B_HR: 0.,  R_EP: 0.,  L_HR:  0.,  R_HR:  0.,   L_LF2: 0.,   R_LF2: 0., 
               R_ET: 0.,  L_EB:  0.,  R_EB:  0.,   L_FR2: 0.,   R_FR2: 0., 
                          L_WR:  0.,  R_WR:  0.,   L_RF:  0.,   R_RF:  0., 
                          L_WFE: 0.,  R_WFE: 0.,   L_RF2: 0.,   R_RF2: 0., 
                          L_WAA: 0.,  R_WAA: 0.,   L_MF:  0.,   R_MF:  0., 
                                                   L_MF2: 0.,   R_MF2: 0. ]

joint_transition: 
  # the joint angles vary at a much higher rate than the biases. for simplicity of
  # analysis we set it to a very large value, practically equivalent to inf.
  # you will not need to change these parameters.
  joint_sigmas: [ 
    # neck       # eyes       # left arm    # right arm    # left hand    # right hand
    B_HN: 100.,  L_EP: 100.,  L_SFE: 100.,  R_SFE: 100.,   L_FR:  100.,   R_FR:  100., 
    B_HT: 100.,  L_ET: 100.,  L_SAA: 100.,  R_SAA: 100.,   L_LF:  100.,   R_LF:  100., 
    B_HR: 100.,  R_EP: 100.,  L_HR:  100.,  R_HR:  100.,   L_LF2: 100.,   R_LF2: 100., 
                 R_ET: 100.,  L_EB:  100.,  R_EB:  100.,   L_FR2: 100.,   R_FR2: 100., 
                              L_WR:  100.,  R_WR:  100.,   L_RF:  100.,   R_RF:  100., 
                              L_WFE: 100.,  R_WFE: 100.,   L_RF2: 100.,   R_RF2: 100., 
                              L_WAA: 100.,  R_WAA: 100.,   L_MF:  100.,   R_MF:  100., 
                                                           L_MF2: 100.,   R_MF2: 100. ]

  # these parameters define how quickly we expect the joint angle bias to vary.
  # the values are the standard deviation of the Gaussian noise injected into the biases.
  # the standard parameters should work well for a wide range of situations, but can 
  # be tuned if necessary.
  # lower values lead to: slower adaptation to biases, smaller max correctable bias,
  # but more steady estimates. larger values lead to opposite behavior.
  bias_sigmas: [
    # neck         # eyes         # left arm      # right arm      # left hand      # right hand
    B_HN: 0.0005,  L_EP: 0.0005,  L_SFE: 0.0005,  R_SFE: 0.0005,   L_FR:  0.0005,   R_FR:  0.0005, 
    B_HT: 0.0005,  L_ET: 0.0005,  L_SAA: 0.0005,  R_SAA: 0.0005,   L_LF:  0.0005,   R_LF:  0.0005, 
    B_HR: 0.0005,  R_EP: 0.0005,  L_HR:  0.0005,  R_HR:  0.0005,   L_LF2: 0.0005,   R_LF2: 0.0005, 
                   R_ET: 0.0005,  L_EB:  0.0005,  R_EB:  0.0005,   L_FR2: 0.0005,   R_FR2: 0.0005, 
                                  L_WR:  0.0005,  R_WR:  0.0005,   L_RF:  0.0005,   R_RF:  0.0005, 
                                  L_WFE: 0.0005,  R_WFE: 0.0005,   L_RF2: 0.0005,   R_RF2: 0.0005, 
                                  L_WAA: 0.0005,  R_WAA: 0.0005,   L_MF:  0.0005,   R_MF:  0.0005, 
                                                                   L_MF2: 0.0005,   R_MF2: 0.0005 ]

  # these parameters specify the forgetting factor the bias is multiplied with at each time step.
  # we picked a forgetting factor of c=0.5 (i.e., after one second the bias is halfed, if
  # there is no evidence to the contrary). our joint angle measurements arrive each
  # dt=1/100 seconds. hence, the forgetting factor for one time step is c^dt=0.993092.
  # the standard parameters should work well for a wide range of situations, but can 
  # be tuned if necessary.
  # lower values lead to: faster convergence of bias to 0 in absence of evidence, 
  # smaller max correctable bias, more steady estimates. larger values lead to opposite behavior.
  #
  # these parameters also serve as switches, determining which joints have to be corrected
  # using visual estimates. if the value is 0 for a given joint, then the bias is always 0,
  # and our estimate will simply be the joint angle measurement. here for instance, we do not 
  # correct the left arm and hand, because they are not being used, and we do not correct the 
  # eyes, because they are irrelevant. furthermore, we do not correct right hand here, 
  # because we might not care so much about precision in the hand, or we might simply trust
  # the joint encoders of the hand enough, such that no correction is necessary.
  bias_factors: [
    # neck           # eyes     # left arm  # right arm       # left hand  # right hand
    B_HN: 0.993092,  L_EP: 0.,  L_SFE: 0.,  R_SFE: 0.993092,  L_FR:  0.,   R_FR:  0., 
    B_HT: 0.993092,  L_ET: 0.,  L_SAA: 0.,  R_SAA: 0.993092,  L_LF:  0.,   R_LF:  0., 
    B_HR: 0.993092,  R_EP: 0.,  L_HR:  0.,  R_HR:  0.993092,  L_LF2: 0.,   R_LF2: 0., 
                     R_ET: 0.,  L_EB:  0.,  R_EB:  0.993092,  L_FR2: 0.,   R_FR2: 0., 
                                L_WR:  0.,  R_WR:  0.993092,  L_RF:  0.,   R_RF:  0., 
                                L_WFE: 0.,  R_WFE: 0.993092,  L_RF2: 0.,   R_RF2: 0., 
                                L_WAA: 0.,  R_WAA: 0.993092,  L_MF:  0.,   R_MF:  0., 
                                                              L_MF2: 0.,   R_MF2: 0. ]                                                      

# below we specify the sampling blocks of the coordinate particle filter [3].
# it is advisable to not sample all dimensions at once, because this is too high
# dimensional. ideally we assign the joints to blocks, such that very related
# joints are in the same block, see below for an example. the block names are 
# just for readability, you can assign any name you like.
sampling_blocks:
  - right_upper_arm: [ R_SFE, R_SAA, R_HR, R_EB ]
  - right_lower_arm: [ R_WR, R_WFE, R_WAA ]
  - neck:            [ B_HN, B_HT, B_HR ]                                    # neck
  - not_corrected:   [ L_EP, L_ET, R_EP, R_ET,                               # eyes
                       L_SFE, L_SAA, L_HR, L_EB, L_WR, L_WFE, L_WAA,         # left arm
                       L_FR, L_LF, L_LF2, L_FR2, L_RF, L_RF2, L_MF, L_MF2,   # left hand
                       R_FR, R_LF, R_LF2, R_FR2, R_RF, R_RF2, R_MF, R_MF2 ]  # right hand

# you can estimate an offset between the nominal camera pose in the kinematic model, 
# and the true camera pose. this allows for precise hand-eye coordination even if 
# the kinematic model has some inaccuracies. the way we do this is by automatically
# inserting 6 virtual joints between the nominal and the true camera, and then 
# estimating the biases in the same way as for the other joints. so the 
# paramters below follow the same logic as the parameters above and will 
# not be explained again.
camera_offset:
  # turn on or off camera offset estimation. the following parameters are only
  # relevant if this is set to true.
  estimate_camera_offset: true 

  joint_observation:
    joint_sigmas: [
      X_JOINT:     0.0, Y_JOINT:   0.0, Z_JOINT:    0.0,
      PITCH_JOINT: 0.0, YAW_JOINT: 0.0, ROLL_JOINT: 0.0
    ]

  joint_transition: 
    joint_sigmas: [
      X_JOINT:     100., Y_JOINT:   100., Z_JOINT:    100.,
      PITCH_JOINT: 100., YAW_JOINT: 100., ROLL_JOINT: 100.
    ]
    bias_sigmas:  [
      X_JOINT:     0.0005, Y_JOINT:   0.0005, Z_JOINT:    0.0005,
      PITCH_JOINT: 0.0005, YAW_JOINT: 0.0005, ROLL_JOINT: 0.0005
    ]
    bias_factors: [
      X_JOINT:     0.993092, Y_JOINT:   0.993092, Z_JOINT:    0.993092,
      PITCH_JOINT: 0.993092, YAW_JOINT: 0.993092, ROLL_JOINT: 0.993092
    ]

  # we also have to specify to which sampling block these virtual joints are added.
  # you can either define a new sampling blocks, as we do here, or add them to
  # an existing sampling block from above, by using an existing name.
  sampling_blocks:
    - camera_offset: [
        X_JOINT, Y_JOINT, Z_JOINT,
        PITCH_JOINT, YAW_JOINT, ROLL_JOINT]


# ROBOT INDEPENDENT SETTINGS ====================================================================

# filter settings
use_gpu: true

cpu:
  # the number of particles if using cpu
  sample_count: 100

gpu:
  # the number of particles if using gpu
  sample_count: 200

  # set true to use custom shader definitions file defined below
  # if set to false, the default shader will be used
  use_custom_shaders: false
  
  vertex_shader_file: /path/to/custom/vertex_shader.vertexshader
  fragment_shader_file: /path/to/custom/fragment_shader.fragmentshader

  # optional (values: none | /path/to/shader)
  geometry_shader_file: none

# if desired, exponential smoothing can be performed on the output
# of the filter. [1.0: no smoothing, 0.0: maximal smoothing]
moving_average_update_rate: 1.0

# particles will only be resampled if their weights are very concentrated.
# this parameter is a measure of this concentration. 
# 0.0: particles will be resampled at each step
# inf: particles will never be resampled
max_kl_divergence: 2.0
  
observation:
  occlusion:
    initial_occlusion_prob: 0.25  # stationary value
    p_occluded_visible: 0.1
    p_occluded_occluded: 0.7
  kinect:
    # these parameters specify the observation model for a single pixel,
    # for details please check [2]  
    tail_weight: 0.01
    model_sigma: 0.003
    sigma_factor: 0.0
  # the time step between two images
  delta_time: 0.033




# [1] Garcia Cifuentes et al. Probabilistic Articulated Real-Time Tracking for Robot Manipulation, ICRA 2017
# [2] Wuthrich et al., Probabilistic Object Tracking Using a Range Camera, IROS 2013
# [3] Wuthrich et al., The Coordinate Particle Filter - A novel Particle Filter for High Dimensional Systems, ICRA 2015

      
    
